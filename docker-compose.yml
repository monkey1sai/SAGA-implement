name: sglang

services:
  # GGUF 模型下載服務
  model_fetch:
    image: lmsysorg/sglang:latest
    container_name: sglang-model-fetch
    env_file: .env
    environment:
      - HF_TOKEN=${HF_TOKEN}
      - HUGGINGFACE_HUB_TOKEN=${HF_TOKEN}
      - SGLANG_GGUF_SOURCE_REPO=${SGLANG_GGUF_SOURCE_REPO:-}
      - SGLANG_GGUF_SOURCE_FILE=${SGLANG_GGUF_SOURCE_FILE:-}
    volumes:
      - ./models:/root/.cache/huggingface
      - ./models_gguf:/models_gguf
    command: |
      bash -lc '
        set -euo pipefail
        mkdir -p /models_gguf
        
        # 若已有模型，直接就緒
        if [ -f /models_gguf/model.gguf ]; then
          echo "[model_fetch] model.gguf already exists, skipping download"
          touch /models_gguf/.ready
          sleep infinity
        fi
        
        # 若未設定來源，跳過
        if [ -z "${SGLANG_GGUF_SOURCE_REPO:-}" ] || [ -z "${SGLANG_GGUF_SOURCE_FILE:-}" ]; then
          echo "[model_fetch] skip (SGLANG_GGUF_SOURCE_REPO or SGLANG_GGUF_SOURCE_FILE not set)"
          touch /models_gguf/.ready
          sleep infinity
        fi
        
        # 下載模型
        rm -f /models_gguf/.ready
        echo "[model_fetch] downloading ${SGLANG_GGUF_SOURCE_REPO} / ${SGLANG_GGUF_SOURCE_FILE}"
        huggingface-cli download "${SGLANG_GGUF_SOURCE_REPO}" "${SGLANG_GGUF_SOURCE_FILE}" \
          --local-dir /tmp/gguf --local-dir-use-symlinks False
        
        cp -f "/tmp/gguf/${SGLANG_GGUF_SOURCE_FILE}" /models_gguf/model.gguf
        touch /models_gguf/.ready
        echo "[model_fetch] ready: /models_gguf/model.gguf"
        sleep infinity
      '
    healthcheck:
      test: [ "CMD", "bash", "-lc", "test -f /models_gguf/.ready" ]
      interval: 10s
      timeout: 5s
      retries: 100
      start_period: 5s

  # SGLang LLM 推論服務
  sglang:
    image: lmsysorg/sglang:latest
    container_name: sglang-server
    runtime: nvidia
    env_file: .env
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - CUDA_VISIBLE_DEVICES=0
      - HF_TOKEN=${HF_TOKEN}
      - HUGGINGFACE_HUB_TOKEN=${HF_TOKEN}
      - SGLANG_API_KEY=${SGLANG_API_KEY}
    ports:
      - "8082:30000"
    volumes:
      - ./models:/root/.cache/huggingface
      - ./models_gguf:/models_gguf
      - ./logs:/app/logs
    depends_on:
      model_fetch:
        condition: service_healthy
    command: |
      bash -lc '
        set -euo pipefail
        python3 -m sglang.launch_server \
          --model-path "${SGLANG_MODEL:-/models_gguf/model.gguf}" \
          --tokenizer-path "${SGLANG_TOKENIZER_PATH:-}" \
          --port 30000 --host 0.0.0.0 \
          --mem-fraction-static "${SGLANG_MEM_FRACTION_STATIC:-0.95}" \
          --context-length "${MAX_MODEL_LEN:-2048}" \
          --api-key "${SGLANG_API_KEY}" \
          --load-format "${SGLANG_LOAD_FORMAT:-auto}" \
          --quantization "${SGLANG_QUANTIZATION:-}" \
          --kv-cache-dtype "${SGLANG_KV_CACHE_DTYPE:-auto}"
      '
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ '0' ]
              capabilities: [ gpu ]
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:30000/health').read()" ]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 180s

  # RAG Service (Hybrid Search: Dense + Sparse + Rerank)
  rag:
    build:
      context: .
      dockerfile: docker/rag.Dockerfile
    container_name: rag-service
    runtime: nvidia
    environment:
      - RAG_EMBEDDING_MODEL=BAAI/bge-m3
      - RAG_EMBEDDING_DEVICE=cuda
      - RAG_VECTOR_DB_PATH=/data/chroma_db
      - RAG_RERANKER_MODEL=BAAI/bge-reranker-base
      - RAG_DEFAULT_TOP_K=5
    volumes:
      - ./data:/data
      - ./models:/root/.cache/huggingface
    ports:
      - "8100:8100"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ '0' ]
              capabilities: [ gpu ]
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8100/health').read()" ]
      interval: 20s
      timeout: 5s
      retries: 10
      start_period: 60s

  # Orchestrator (Web ↔ SGLang ↔ RAG 協調器)
  orchestrator:
    build:
      context: .
      dockerfile: docker/orchestrator.Dockerfile
    container_name: orchestrator
    depends_on:
      sglang:
        condition: service_healthy
    environment:
      - ORCH_HOST=0.0.0.0
      - ORCH_PORT=9100
      - ORCH_API_KEY=${ORCH_API_KEY:-}
      - SGLANG_BASE_URL=http://sglang:30000
      - SGLANG_API_KEY=${SGLANG_API_KEY}
      - SGLANG_MODEL=${SGLANG_MODEL:-/models_gguf/model.gguf}
      - SGLANG_MAX_TOKENS=${SGLANG_MAX_TOKENS:-512}
      - SGLANG_SYSTEM_PROMPT=${SGLANG_SYSTEM_PROMPT:-請用繁體中文回答}
      - SGLANG_TEMPERATURE=${SGLANG_TEMPERATURE:-0.6}
      - SGLANG_TOP_P=${SGLANG_TOP_P:-0.9}
      - SGLANG_TOP_K=${SGLANG_TOP_K:-20}
      - SGLANG_REPETITION_PENALTY=${SGLANG_REPETITION_PENALTY:-1.15}
      - RAG_SERVICE_URL=http://rag:8100
    ports:
      - "9100:9100"
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:9100/healthz').read()" ]
      interval: 20s
      timeout: 5s
      retries: 10
      start_period: 10s

  # SAGA Server (Workflow + Observability)
  saga_server:
    build:
      context: .
      dockerfile: docker/saga_server.Dockerfile
    container_name: saga-server
    env_file: .env
    environment:
      - SGLANG_URL=http://sglang-server:30000/v1/chat/completions
      - SGLANG_BASE_URL=http://sglang-server:30000
      - SGLANG_API_KEY=${SGLANG_API_KEY}
      - SAGA_USE_LLM_MODULES=true
      - SAGA_USE_GROQ=${SAGA_USE_GROQ:-false}
      - GROQ_API_KEY=${GROQ_API_KEY:-}
      - GROQ_MODEL=${GROQ_MODEL:-openai/gpt-oss-120b}
    ports:
      - "9200:9200"
    restart: unless-stopped
    depends_on:
      sglang:
        condition: service_healthy
    healthcheck:
      test: [ "CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:9200/healthz').read()" ]
      interval: 20s
      timeout: 5s
      retries: 10
      start_period: 10s

  # Web (Nginx 反向代理 + 前端靜態檔)
  web:
    build:
      context: .
      dockerfile: docker/web.Dockerfile
    container_name: web
    depends_on:
      orchestrator:
        condition: service_healthy
      rag:
        condition: service_healthy
      sglang:
        condition: service_healthy
      saga_server:
        condition: service_healthy
    ports:
      - "8080:80"
    environment:
      - ORCHESTRATOR_UPSTREAM=${ORCHESTRATOR_UPSTREAM:-orchestrator:9100}
      - SAGA_UPSTREAM=${SAGA_UPSTREAM:-saga_server:9200}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "wget", "-qO-", "http://127.0.0.1/healthz" ]
      interval: 20s
      timeout: 5s
      retries: 10
      start_period: 5s

volumes:
  rag-data:
