version: '3.8'

services:
  # ===========================================
  # vLLM 推論服務 (RTX 4060 Ti 8GB 優化版)
  # ===========================================
  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm-server
    runtime: nvidia
    dns:
      - 8.8.8.8
      - 8.8.4.4
      - 1.1.1.1
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - VLLM_API_KEY=${VLLM_API_KEY}
      - HF_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
      - VLLM_ATTENTION_BACKEND=FLASH_ATTN
    ports:
      - "8081:8000"
    volumes:
      # 模型快取目錄 (避免重複下載)
      - ./models:/root/.cache/huggingface
      # 日誌目錄
      - ./logs:/app/logs
    command: >
      --model Qwen/Qwen2.5-1.5B-Instruct --max-model-len 1024 --gpu-memory-utilization 0.6 --max-num-seqs 8 --max-num-batched-tokens 2048 --enable-prefix-caching --dtype half --enforce-eager --api-key ${VLLM_API_KEY} --host 0.0.0.0 --port 8000
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ '0' ]
              capabilities: [ gpu ]
        limits:
          memory: 16G
    restart: always
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  # ===========================================
  # Nginx 反向代理 (HTTPS + Load Balancer)
  # ===========================================
  nginx:
    image: nginx:alpine
    container_name: vllm-proxy
    ports:
      - "443:443"
      - "80:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/certs:/etc/nginx/certs:ro
      - ./nginx/logs:/var/log/nginx
    depends_on:
      vllm:
        condition: service_healthy
    restart: always
    healthcheck:
      test: [ "CMD", "nginx", "-t" ]
      interval: 30s
      timeout: 10s
      retries: 3

  # ===========================================
  # Prometheus 監控 (可選)
  # ===========================================
  prometheus:
    image: prom/prometheus:latest
    container_name: vllm-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=7d'
    restart: always
    profiles:
      - monitoring

  # ===========================================
  # Grafana 視覺化 (可選)
  # ===========================================
  grafana:
    image: grafana/grafana:latest
    container_name: vllm-grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin123}
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
    depends_on:
      - prometheus
    restart: always
    profiles:
      - monitoring

volumes:
  prometheus_data:
  grafana_data:


networks:
  default:
    name: vllm-network
    driver: bridge
