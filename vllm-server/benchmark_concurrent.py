"""
vLLM Concurrent Throughput Benchmark (Streaming)
æ¸¬è©¦ vLLM åœ¨é«˜ä½µç™¼ä¸‹çš„é¦–å­—å»¶é² (TTFT) èˆ‡ç¸½ç”Ÿæˆé€Ÿåº¦
"""

import asyncio
import time
import json
import aiohttp
import statistics
from typing import List, Dict, Any

# ============== é…ç½® ==============
VLLM_URL = "http://localhost:8081/v1/chat/completions"
API_KEY = "your-secure-api-key-here"
MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"

# æ¸¬è©¦åƒæ•¸
CONCURRENT_REQUESTS = 20  # æ¨¡æ“¬å¤šå°‘äººåŒæ™‚è«‹æ±‚
MAX_TOKENS = 128          # æ¯å€‹è«‹æ±‚ç”Ÿæˆçš„ token æ•¸
TOTAL_REQUESTS = 40       # ç¸½å…±ç™¼é€å¤šå°‘å€‹è«‹æ±‚

PROMPT = "Write a short summary about the history of the internet."

async def make_stream_request(session: aiohttp.ClientSession, request_id: int) -> Dict[str, Any]:
    """ç™¼é€å–®å€‹ä¸²æµ (Stream) è«‹æ±‚ä¸¦æ¸¬é‡æ™‚é–“"""
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": MODEL_NAME,
        "messages": [{"role": "user", "content": PROMPT}],
        "max_tokens": MAX_TOKENS,
        "temperature": 0.7,
        "stream": True  # <--- é‡è¦ï¼šé–‹å•Ÿä¸²æµæ¨¡å¼
    }
    
    start_time = time.perf_counter()
    first_token_time = None
    token_count = 0
    
    try:
        async with session.post(VLLM_URL, headers=headers, json=payload) as response:
            if response.status != 200:
                return {"error": f"Status {response.status}", "latency": 0}
            
            async for line in response.content:
                line = line.decode('utf-8').strip()
                if line.startswith("data: ") and line != "data: [DONE]":
                    try:
                        # è¨˜éŒ„æ”¶åˆ°ç¬¬ä¸€å€‹ token çš„æ™‚é–“ (TTFT)
                        if first_token_time is None:
                            first_token_time = time.perf_counter()
                        
                        data = json.loads(line[6:]) # remove "data: "
                        if "choices" in data and len(data["choices"]) > 0:
                            delta = data["choices"][0].get("delta", {})
                            if "content" in delta and delta["content"]:
                                token_count += 1
                    except:
                        pass
                        
        end_time = time.perf_counter()
        
        # è¨ˆç®—æŒ‡æ¨™
        ttft = (first_token_time - start_time) if first_token_time else (end_time - start_time)
        gen_time = (end_time - first_token_time) if first_token_time else 0
        total_time = end_time - start_time
        
        return {
            "id": request_id,
            "ttft": ttft,                # é¦–å­—å»¶é² (åæ‡‰æ™‚é–“)
            "gen_time": gen_time,        # ç”Ÿæˆæ™‚é–“ (è¼¸å‡ºéç¨‹)
            "total_time": total_time,    # ç¸½æ™‚é–“
            "tokens": token_count,
            "tps": token_count / total_time if total_time > 0 else 0
        }
        
    except Exception as e:
        return {"error": str(e), "latency": 0}

async def run_benchmark():
    print("=" * 60)
    print("ğŸš€ vLLM Concurrent Streaming Benchmark")
    print("=" * 60)
    print(f"Model: {MODEL_NAME}")
    print(f"Concurrency: {CONCURRENT_REQUESTS} users")
    print(f"Total Requests: {TOTAL_REQUESTS}")
    print(f"Max Tokens: {MAX_TOKENS}")
    print("=" * 60)
    print("\nStarting benchmark... (measuring TTFT and Generation Time)\n")

    async with aiohttp.ClientSession() as session:
        tasks = []
        global_start = time.perf_counter()
        
        for i in range(TOTAL_REQUESTS):
            tasks.append(make_stream_request(session, i))
        
        results = await asyncio.gather(*tasks)
        global_end = time.perf_counter()

    valid_results = [r for r in results if "error" not in r]
    if not valid_results:
        print("âŒ All requests failed!")
        return

    # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
    avg_ttft = statistics.mean(r["ttft"] for r in valid_results)
    avg_gen_time = statistics.mean(r["gen_time"] for r in valid_results)
    avg_total_time = statistics.mean(r["total_time"] for r in valid_results)
    
    total_tokens = sum(r["tokens"] for r in valid_results)
    global_duration = global_end - global_start
    system_throughput = total_tokens / global_duration

    print("=" * 60)
    print("ğŸ“Š DETAILED LATENCY ANALYSIS")
    print("=" * 60)
    print(f"1ï¸âƒ£  Time to First Token (TTFT) / åæ‡‰æ™‚é–“")
    print(f"    (åŒ…å«æ’éšŠ, Prompt è™•ç†, é–‹å§‹æ€è€ƒ)")
    print(f"    Average: {avg_ttft:.4f} s")
    print(f"    Min:     {min(r['ttft'] for r in valid_results):.4f} s")
    print(f"    Max:     {max(r['ttft'] for r in valid_results):.4f} s")
    print("-" * 60)
    
    print(f"2ï¸âƒ£  Generation Time / è¼¸å‡ºç”Ÿæˆæ™‚é–“")
    print(f"    (å¾åå‡ºç¬¬ä¸€å€‹å­—åˆ°çµæŸ)")
    print(f"    Average: {avg_gen_time:.4f} s")
    print("-" * 60)
    
    print(f"3ï¸âƒ£  Total Request Time / ç¸½è€—æ™‚")
    print(f"    Average: {avg_total_time:.4f} s")
    print("=" * 60)
    
    print(f"ğŸ”¥ System Throughput: {system_throughput:.2f} tokens/s")
    print(f"âœ… Successful Req:    {len(valid_results)}/{TOTAL_REQUESTS}")
    print("=" * 60)

if __name__ == "__main__":
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(run_benchmark())
